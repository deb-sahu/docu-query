# DocuQuery Environment Configuration
# Copy this file to .env and configure your settings

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Choose your LLM provider: "gemini" or "ollama"
LLM_PROVIDER=gemini

# -----------------------------------------------------------------------------
# Option A: Google Gemini (Recommended - Free tier available)
# Get your free API key at: https://aistudio.google.com/apikey
# -----------------------------------------------------------------------------
GEMINI_API_KEY=your_gemini_api_key_here
GEMINI_MODEL=gemini-2.0-flash

# -----------------------------------------------------------------------------
# Option B: Ollama (100% local, no API key needed)
# Install Ollama from: https://ollama.ai
# Then run: ollama pull llama3.2
# -----------------------------------------------------------------------------
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2

# =============================================================================
# Embedding & Search Configuration (Optional)
# =============================================================================

# Sentence transformer model for embeddings (runs locally)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# Text chunking settings
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# Number of context chunks to retrieve for answering
TOP_K_RESULTS=4

